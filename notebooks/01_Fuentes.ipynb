{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd2a5ad9",
   "metadata": {},
   "source": [
    "# Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b9bfdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "from dotenv import dotenv_values\n",
    "import utils as u\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "pd.set_option('max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a92ad5",
   "metadata": {},
   "source": [
    "### **GOOGLE MAPS:** Nearby Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e664b9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dotenv_values(\".env\") # Archivo de API Key oculto el archivo .env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dd526a",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Este bucle, nos ayuda a extraer desde la API de Google Maps, los distintos IDs (restaurantes), que existe en la ciudad dentro de la M30.\n",
    "\n",
    "El bucle consiste en:\n",
    "\n",
    "Determinar los valores máximos y minimos de la latitud y longitud. Mirar los puntos extremos de cada medida.\n",
    "\n",
    "El bucle va iterar por cada latitud y logitud ira aumentando en un step de 0.01. Cada uno de estas lat y lon realizará la consulta en la API\n",
    "con un radio de 1000 metros y extraerá todos los id, incluyendo los de otras páginas (next_page_token) Google Maps solo tiene un max de 3 páginas.\n",
    "\n",
    "Por último, hacemos un list comprehension en diccionarios para crear un diccionario que la clave es el place_id.\n",
    "\n",
    "'''\n",
    "\n",
    "lugares = []\n",
    "\n",
    "norte = 40.485657\n",
    "sur = 40.389104\n",
    "este = -3.659151\n",
    "oeste = -3.749617\n",
    "step = 0.01\n",
    "\n",
    "\n",
    "for lat in np.arange(sur, norte, step):\n",
    "    for lon in np.arange(oeste, este, step):\n",
    "        url = 'https://maps.googleapis.com/maps/api/place/nearbysearch/json'\n",
    "        params = {'location': f'{lat},{lon}',\n",
    "          'radius': 1000,\n",
    "          'type': 'restaurant',\n",
    "          'key':config['access_key']}\n",
    "\n",
    "        for page in range(3):\n",
    "            res = requests.get(url, params=params)\n",
    "            #print(res)\n",
    "            data = res.json()\n",
    "            lugares += data.get('results', [])\n",
    "            next_page_token = data.get('next_page_token')\n",
    "            if next_page_token is not None:\n",
    "                time.sleep(2)\n",
    "                params = {'pagetoken': next_page_token, 'key': config['access_key']}\n",
    "            else:\n",
    "                break\n",
    "\n",
    "lugares_unicos = {lugar['place_id']: lugar for lugar in lugares}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5dddb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/raw/lugares_madrid_raw.csv', 'w') as file: # Guardamos el archivo en raw\n",
    "     file.write(json.dumps(lugares_unicos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a63a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "lugaresdf = u.json_to_dataframe(lugares_unicos) # Transformamos el json a un Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6499d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "lugaresdf.to_csv('../data/raw/lugares_madrid.csv', index=False) # Guardamos el archivo final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f28c9b",
   "metadata": {},
   "source": [
    "### **GOOGLE MAPS:** Places Detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b00cb3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nombre</th>\n",
       "      <th>id</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Arrocería Aynaelda</td>\n",
       "      <td>ChIJu0clrRCIQQ0R00GSTWpDKZQ</td>\n",
       "      <td>40.395986</td>\n",
       "      <td>-3.754164</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               nombre                           id        lat       lon\n",
       "0  Arrocería Aynaelda  ChIJu0clrRCIQQ0R00GSTWpDKZQ  40.395986 -3.754164"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lugaresdf = pd.read_csv('../data/raw/lugares_madrid.csv')\n",
    "lugaresdf.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59229863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error con ID ChIJpYflyTYmQg0RVix8ZBBSzMI: HTTPSConnectionPool(host='maps.googleapis.com', port=443): Read timed out. (read timeout=10)\n"
     ]
    }
   ],
   "source": [
    "''' \n",
    "Este bucle, nos ayuda a extraer desde la API de Google Maps de los IDs (restaurantes) antes sacados, el detalle de cada lugar.\n",
    "\n",
    "El bucle consiste en:\n",
    "\n",
    "Iterar por cada place_id, consultar en el api los detalles de los campos incluidos en la variable fields.\n",
    "Y por último, gardarlos en una lista detalle_sitios.\n",
    "\n",
    "'''\n",
    "\n",
    "fields = \",\".join([\"place_id\", \"formatted_address\",\n",
    "    \"wheelchair_accessible_entrance\",\"opening_hours\",\n",
    "    \"delivery\", \"dine_in\", \"editorial_summary\",\n",
    "    \"price_level\", \"rating\", \"reservable\", \"reviews\",\n",
    "    \"serves_beer\", \"serves_breakfast\", \"serves_brunch\",\n",
    "    \"serves_dinner\", \"serves_lunch\", \"serves_vegetarian_food\",\n",
    "    \"serves_wine\", \"takeout\", \"user_ratings_total\",\"types\"\n",
    "])\n",
    "\n",
    "detalle_sitios = []\n",
    "\n",
    "for i in lugaresdf['id']:\n",
    "    url = 'https://maps.googleapis.com/maps/api/place/details/json'\n",
    "    \n",
    "    params = {'place_id': i,\n",
    "            'fields': fields,\n",
    "            'language': 'en',\n",
    "            'key':config['access_key']}\n",
    "\n",
    "    try:\n",
    "        details = requests.get(url, params=params, timeout=10)\n",
    "        details.raise_for_status()\n",
    "        resultado = details.json().get('result', {})\n",
    "        detalle_sitios.append(resultado)\n",
    "    except Exception as e:\n",
    "        print(f\"Error con ID {i}: {e}\")\n",
    "        detalle_sitios.append({})\n",
    "    time.sleep(0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5830e145",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/raw/detalle_sitios_raw.csv', 'w') as file: # Guardamos el archivo en raw\n",
    "     file.write(json.dumps(detalle_sitios))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83e6896f",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Guardamos los reviews en otro dataframe para procesar con NLP.\n",
    "'''\n",
    "rev = {'place_id':[],\n",
    "       'rating_rev':[],\n",
    "       'text':[]}\n",
    "\n",
    "for i in detalle_sitios:\n",
    "    try:\n",
    "        reviews = i['reviews']\n",
    "        \n",
    "        for j in reviews:\n",
    "            rev['place_id'].append(i.get('place_id')) \n",
    "            rev['rating_rev'].append(j.get('rating', ''))\n",
    "            rev['text'].append(j.get('text', ''))\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "df_reviews = pd.DataFrame(rev) # Guardamos el resultado en un DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6200e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews.to_csv('../data/raw/detalle_reviews.csv', index=False) # Guardamos el archivo final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "23c8b1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Este bucle, nos ayuda a extraer desde el json del detalle de sitios los distintos atributos.\n",
    "\n",
    "'''\n",
    "\n",
    "df_detalle = {  'place_id':[],\n",
    "                'dine_in':[],\n",
    "                'summary':[],\n",
    "                'address':[],\n",
    "                'price_level':[],\n",
    "                'rating':[],\n",
    "                'reservable':[],\n",
    "                'serves_beer':[],\n",
    "                'serves_breakfast':[],\n",
    "                'serves_brunch':[],\n",
    "                'serves_dinner':[],\n",
    "                'serves_lunch':[],\n",
    "                'serves_vegetarian_food':[],\n",
    "                'serves_wine':[],\n",
    "                'takeout':[],\n",
    "                'user_ratings_total':[],\n",
    "                'delivery':[],\n",
    "                'weelchair':[],\n",
    "                'hours_open':[],\n",
    "                'num_days_open':[],\n",
    "                'open_weekends':[],\n",
    "                'types':[]                          \n",
    "                }\n",
    "\n",
    "for i in detalle_sitios:\n",
    "    df_detalle['place_id'].append(i.get('place_id'))\n",
    "    df_detalle['dine_in'].append(i.get('dine_in', np.nan))\n",
    "    df_detalle['address'].append(i.get('formatted_address', ''))\n",
    "    df_detalle['rating'].append(i.get('rating', np.nan))\n",
    "    df_detalle['user_ratings_total'].append(i.get('user_ratings_total', np.nan))\n",
    "    df_detalle['delivery'].append(i.get('delivery', np.nan))\n",
    "    df_detalle['summary'].append(i.get('editorial_summary', {}).get('overview', ''))\n",
    "    df_detalle['price_level'].append(i.get('price_level', np.nan))\n",
    "    df_detalle['reservable'].append(i.get('reservable', np.nan))\n",
    "    df_detalle['serves_beer'].append(i.get('serves_beer', np.nan))\n",
    "    df_detalle['serves_breakfast'].append(i.get('serves_breakfast', np.nan))\n",
    "    df_detalle['serves_brunch'].append(i.get('serves_brunch', np.nan))\n",
    "    df_detalle['serves_dinner'].append(i.get('serves_dinner', np.nan))\n",
    "    df_detalle['serves_lunch'].append(i.get('serves_lunch', np.nan))\n",
    "    df_detalle['serves_vegetarian_food'].append(i.get('serves_vegetarian_food', np.nan))\n",
    "    df_detalle['serves_wine'].append(i.get('serves_wine', np.nan))\n",
    "    df_detalle['takeout'].append(i.get('takeout', np.nan))\n",
    "    df_detalle['weelchair'].append(i.get('wheelchair_accessible_entrance', np.nan))\n",
    "    df_detalle['types'].append(i.get('types', ''))\n",
    "\n",
    "\n",
    "    total_hours = 0\n",
    "    open_weekend = 0\n",
    "    num_day_open = set()\n",
    "    try:\n",
    "        periods = i['opening_hours']['periods']\n",
    "        for period in periods:\n",
    "            open_day = period['open']['day']\n",
    "            open_time = period['open']['time']\n",
    "            close_time = period['close']['time']\n",
    "\n",
    "            open_hour = int(open_time[:2])\n",
    "            open_minute = int(open_time[2:])\n",
    "\n",
    "            close_hour = int(close_time[:2])\n",
    "            close_minute = int(close_time[2:])\n",
    "\n",
    "            open_dt = datetime.strptime(f\"{open_hour}:{open_minute}\", \"%H:%M\")\n",
    "            close_dt = datetime.strptime(f\"{close_hour}:{close_minute}\", \"%H:%M\")\n",
    "\n",
    "            if close_dt <= open_dt:\n",
    "                close_dt += timedelta(days=1)\n",
    "\n",
    "            hours_open = (close_dt - open_dt).total_seconds() / 3600\n",
    "            total_hours += hours_open\n",
    "            num_day_open.add(open_day)\n",
    "\n",
    "            if open_day == 0 or open_day == 6:\n",
    "                open_weekend += 1\n",
    "            \n",
    "    except:\n",
    "        total_hours = np.nan\n",
    "\n",
    "    num_day_open = len(num_day_open)\n",
    "    df_detalle['num_days_open'].append(num_day_open)\n",
    "    df_detalle['hours_open'].append(total_hours)\n",
    "    if open_weekend>=2:\n",
    "        df_detalle['open_weekends'].append(True) \n",
    "    else:\n",
    "        df_detalle['open_weekends'].append(False) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2541d849",
   "metadata": {},
   "outputs": [],
   "source": [
    "detalle_sitios_df = pd.DataFrame(df_detalle) #Guardamos el resultado en un DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "84235a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "detalle_sitios_df.to_csv('../data/raw/detalle_sitios.csv', index=False) # Guardamos el archivo final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d641427",
   "metadata": {},
   "source": [
    "### **Open StreetMap**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9dd8a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "''' \n",
    "Consulta de Open StreetMap de los restaurantes de España.\n",
    "\n",
    "'''\n",
    "\n",
    "overpass_url = \"http://overpass-api.de/api/interpreter\"\n",
    "\n",
    "\n",
    "overpass_query = \"\"\" \n",
    "[out:json][timeout:60];\n",
    "\n",
    "area[\"name\"=\"España\"][\"admin_level\"=\"2\"]->.spain;\n",
    "\n",
    "(\n",
    "  node[\"amenity\"=\"restaurant\"](area.spain);\n",
    "  way[\"amenity\"=\"restaurant\"](area.spain);\n",
    "  relation[\"amenity\"=\"restaurant\"](area.spain);\n",
    ");\n",
    "\n",
    "out center;\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "response = requests.get(overpass_url, params={'data': overpass_query})\n",
    "print(response)\n",
    "data = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f6fbc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Este bucle, nos ayuda a extraer desde el json del detalle de sitios los distintos atributos.\n",
    "\n",
    "'''\n",
    "\n",
    "street = {'nombre':[],\n",
    "          'postal':[],\n",
    "          'calle':[],\n",
    "          'numero':[],\n",
    "          'cocina':[],\n",
    "          'lat':[],\n",
    "          'lon':[],\n",
    "          'wheelchair':[]}\n",
    "\n",
    "for i in data['elements']:\n",
    "    street['nombre'].append(i['tags'].get('name', ''))\n",
    "    street['postal'].append(i['tags'].get('addr:postcode', ''))\n",
    "    street['calle'].append(i['tags'].get('addr:street', ''))\n",
    "    street['numero'].append(i['tags'].get('addr:housenumber', ''))\n",
    "    street['cocina'].append(i['tags'].get('cuisine', ''))\n",
    "    street['lat'].append(i.get('lat', ''))\n",
    "    street['lon'].append(i.get('lon', ''))\n",
    "    street['wheelchair'].append(i['tags'].get('wheelchair', ''))\n",
    "\n",
    "street_df = pd.DataFrame(street)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7f7610f",
   "metadata": {},
   "outputs": [],
   "source": [
    "street_df.to_csv('../data/raw/sitios_streetmap.csv', index=False) # Guardamos el archivo final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298b8968",
   "metadata": {},
   "source": [
    "### **Ayuntamiento Madrid**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90b9e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Datos del Ayuntamiento de Madrid con distintos KPI por barrio, filtramos los datos del 2024.\n",
    "'''\n",
    "\n",
    "df_barrios = pd.read_csv('../data/raw/panel_indicadores_distritos_barrios.csv', sep=';', index_col='Orden')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "230fe6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_barrios['indicador_completo'] = df_barrios['indicador_completo'].str.strip() # Quitamos los espacios en blanco de los indicadores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "2b40a0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_barrios = df_barrios[(df_barrios['cod_distrito'].notna())&(df_barrios['cod_barrio'].notna())] # quitamos los kpi que no estan por barrio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7a2461",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Muchos de estos indicadores, viene repetidos por año y barrio por lo que creamos este proceso que primero:\n",
    "- Calculamos para cada barrio e indicador el periodo panel max.\n",
    "- Filtramos el dataset para quedarnos solo con los registros de su fecha max con el merge.\n",
    "- Calculamos de este nuevo dataset filtrado para cada barrio e indicador el año maximo.\n",
    "- Filtramos el dataset para quedarnos solo con los registros de su año max con el merge.\n",
    "\n",
    "De esta forma logramos tenemos para cada barrio e indicador la informacion las reciente.\n",
    "'''\n",
    "\n",
    "\n",
    "val = df_barrios.groupby(['cod_barrio', 'indicador_completo'])[['Periodo panel']].max().reset_index()\n",
    "\n",
    "df_barrios = pd.merge(df_barrios, val, how='inner', \n",
    "                      left_on=['cod_barrio', 'indicador_completo', 'Periodo panel'],\n",
    "                        right_on=['cod_barrio', 'indicador_completo', 'Periodo panel'])\n",
    "\n",
    "val2 = df_barrios.groupby(['cod_barrio', 'indicador_completo'])[['año']].max().reset_index()\n",
    "\n",
    "df_barrios = pd.merge(df_barrios, val2, how='inner', \n",
    "                      left_on=['cod_barrio', 'indicador_completo', 'año'],\n",
    "                        right_on=['cod_barrio', 'indicador_completo', 'año'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "531b6606",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Nos quedamos con las columnas que nos interesan.\n",
    "'''\n",
    "\n",
    "df_barrios.drop(['Periodo panel', 'ciudad',\n",
    "                 'fecha_indicador', 'fuente_indicador', \n",
    "                 'categoría_1', 'categoría_2', 'indicador_nivel1',\n",
    "                 'indicador_nivel2', 'indicador_nivel3'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "4bcbf206",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_barrios.to_csv('../data/raw/kpi_barrios_madrid.csv', index=False) # Guardamos el archivo final"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
